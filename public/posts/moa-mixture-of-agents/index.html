<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>MoA: Mixture of Agents | Abstract of an AI Agent Age</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="The proposed MoA architecture
The proposed architecture contains two main components: the proposers and the aggregators, both of which are LLMs that excel in their respective aspect of collaboration. for example GPT-4o, Qwen1.5, LLaMA-3 emerged as a versatile model effective in both assisting and aggregating tasks. In contrast, WizardLM demonstrated excellent performance as a proposer model but struggled to maintain its effectiveness in aggregating responses from other models.
learn more at:">
    <meta name="generator" content="Hugo 0.147.3">
    
    
    
      <meta name="robots" content="index, follow">
    
    

    
<link rel="stylesheet" href="/public/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >




    


    
      

    

    

    
      <link rel="canonical" href="https://qidu.github.io/public/posts/moa-mixture-of-agents/">
    

    <meta property="og:url" content="https://qidu.github.io/public/posts/moa-mixture-of-agents/">
  <meta property="og:site_name" content="Abstract of an AI Agent Age">
  <meta property="og:title" content="MoA: Mixture of Agents">
  <meta property="og:description" content="The proposed MoA architecture
The proposed architecture contains two main components: the proposers and the aggregators, both of which are LLMs that excel in their respective aspect of collaboration. for example GPT-4o, Qwen1.5, LLaMA-3 emerged as a versatile model effective in both assisting and aggregating tasks. In contrast, WizardLM demonstrated excellent performance as a proposer model but struggled to maintain its effectiveness in aggregating responses from other models.
learn more at:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-14T19:34:20+08:00">
    <meta property="article:modified_time" content="2025-05-14T19:34:20+08:00">

  <meta itemprop="name" content="MoA: Mixture of Agents">
  <meta itemprop="description" content="The proposed MoA architecture
The proposed architecture contains two main components: the proposers and the aggregators, both of which are LLMs that excel in their respective aspect of collaboration. for example GPT-4o, Qwen1.5, LLaMA-3 emerged as a versatile model effective in both assisting and aggregating tasks. In contrast, WizardLM demonstrated excellent performance as a proposer model but struggled to maintain its effectiveness in aggregating responses from other models.
learn more at:">
  <meta itemprop="datePublished" content="2025-05-14T19:34:20+08:00">
  <meta itemprop="dateModified" content="2025-05-14T19:34:20+08:00">
  <meta itemprop="wordCount" content="93">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="MoA: Mixture of Agents">
  <meta name="twitter:description" content="The proposed MoA architecture
The proposed architecture contains two main components: the proposers and the aggregators, both of which are LLMs that excel in their respective aspect of collaboration. for example GPT-4o, Qwen1.5, LLaMA-3 emerged as a versatile model effective in both assisting and aggregating tasks. In contrast, WizardLM demonstrated excellent performance as a proposer model but struggled to maintain its effectiveness in aggregating responses from other models.
learn more at:">

      
    
	
  </head><body class="ma0 avenir bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/public/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Abstract of an AI Agent Age
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">MoA: Mixture of Agents</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-05-14T19:34:20+08:00">May 14, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><code>The proposed MoA architecture</code></p>
<p>The proposed architecture contains two main components: the proposers and the aggregators, both of which are LLMs that excel in their respective aspect of collaboration. for example GPT-4o, Qwen1.5, LLaMA-3 emerged as a versatile model effective in both assisting and aggregating tasks. In contrast, <code>WizardLM</code> demonstrated excellent performance as a proposer model but struggled to maintain its effectiveness in aggregating responses from other models.</p>
<p>learn more at:</p>
<ul>
<li><a href="https://medium.com/@abdulrahmanrihan/mixture-of-agents-moa-a-better-moe-33683151beec">Mixture of Agents (MoA): a better MoE?</a></li>
<li><a href="https://arxiv.org/html/2406.04692v1">Mixture-of-Agents Enhances Large Language Model Capabilities</a></li>
<li><a href="https://arxiv.org/pdf/2304.12244">WizardLM: Empowering Large Language Models to Follow Complex Instructions</a></li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="https://qidu.github.io/public/" >
    &copy;  Abstract of an AI Agent Age 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
