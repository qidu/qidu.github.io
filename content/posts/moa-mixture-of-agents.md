+++
date = '2025-05-14T19:34:20+08:00'
draft = false
title = 'MoA: Mixture of Agents'
+++

`The proposed MoA architecture`

The proposed architecture contains two main components: the proposers and the aggregators, both of which are LLMs that excel in their respective aspect of collaboration. for example GPT-4o, Qwen1.5, LLaMA-3 emerged as a versatile model effective in both assisting and aggregating tasks. In contrast, `WizardLM` demonstrated excellent performance as a proposer model but struggled to maintain its effectiveness in aggregating responses from other models.

learn more at:
- [Mixture of Agents (MoA): a better MoE?](https://medium.com/@abdulrahmanrihan/mixture-of-agents-moa-a-better-moe-33683151beec)
- [Mixture-of-Agents Enhances Large Language Model Capabilities](https://arxiv.org/html/2406.04692v1)
